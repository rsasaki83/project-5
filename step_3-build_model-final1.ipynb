{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lib.project_5 import load_data_from_database, make_data_dict, general_model, general_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: EACH OF THESE SHOULD BE WRITTEN SOLELY WITH REGARD TO STEP 3 - Build Model**\n",
    "\n",
    "### Domain and Data\n",
    "\n",
    "**TODO:** Write a simple statement about the domain of your problem and the dataset upon which you will be working. \n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "**TODO:** Write a simple problem statement with regard to building your model. As this is the final step, you may want to be a bit more \"global\" here.\n",
    "\n",
    "### Solution Statement\n",
    "\n",
    "**TODO:** Write a simple solution statement with regard to building your model.\n",
    "\n",
    "### Metric\n",
    "\n",
    "**TODO**: Write a statement about the metric you will be using. \n",
    "\n",
    "### Benchmark\n",
    "\n",
    "**TODO**: This should refer to Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Implement the following code pipeline using the functions you write in `lib/project_5.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/build_model.png\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>feat_000</th>\n",
       "      <th>feat_001</th>\n",
       "      <th>feat_002</th>\n",
       "      <th>feat_003</th>\n",
       "      <th>feat_004</th>\n",
       "      <th>feat_005</th>\n",
       "      <th>feat_006</th>\n",
       "      <th>feat_007</th>\n",
       "      <th>feat_008</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_491</th>\n",
       "      <th>feat_492</th>\n",
       "      <th>feat_493</th>\n",
       "      <th>feat_494</th>\n",
       "      <th>feat_495</th>\n",
       "      <th>feat_496</th>\n",
       "      <th>feat_497</th>\n",
       "      <th>feat_498</th>\n",
       "      <th>feat_499</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>485</td>\n",
       "      <td>477</td>\n",
       "      <td>537</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>471</td>\n",
       "      <td>491</td>\n",
       "      <td>476</td>\n",
       "      <td>475</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>477</td>\n",
       "      <td>485</td>\n",
       "      <td>511</td>\n",
       "      <td>485</td>\n",
       "      <td>481</td>\n",
       "      <td>479</td>\n",
       "      <td>475</td>\n",
       "      <td>496</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>483</td>\n",
       "      <td>458</td>\n",
       "      <td>460</td>\n",
       "      <td>487</td>\n",
       "      <td>587</td>\n",
       "      <td>475</td>\n",
       "      <td>526</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>...</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>338</td>\n",
       "      <td>513</td>\n",
       "      <td>486</td>\n",
       "      <td>483</td>\n",
       "      <td>492</td>\n",
       "      <td>510</td>\n",
       "      <td>517</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>487</td>\n",
       "      <td>542</td>\n",
       "      <td>499</td>\n",
       "      <td>468</td>\n",
       "      <td>448</td>\n",
       "      <td>471</td>\n",
       "      <td>442</td>\n",
       "      <td>478</td>\n",
       "      <td>480</td>\n",
       "      <td>...</td>\n",
       "      <td>481</td>\n",
       "      <td>492</td>\n",
       "      <td>650</td>\n",
       "      <td>506</td>\n",
       "      <td>501</td>\n",
       "      <td>480</td>\n",
       "      <td>489</td>\n",
       "      <td>499</td>\n",
       "      <td>498</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>480</td>\n",
       "      <td>491</td>\n",
       "      <td>510</td>\n",
       "      <td>485</td>\n",
       "      <td>495</td>\n",
       "      <td>472</td>\n",
       "      <td>417</td>\n",
       "      <td>474</td>\n",
       "      <td>502</td>\n",
       "      <td>...</td>\n",
       "      <td>480</td>\n",
       "      <td>474</td>\n",
       "      <td>572</td>\n",
       "      <td>454</td>\n",
       "      <td>469</td>\n",
       "      <td>475</td>\n",
       "      <td>482</td>\n",
       "      <td>494</td>\n",
       "      <td>461</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>484</td>\n",
       "      <td>502</td>\n",
       "      <td>528</td>\n",
       "      <td>489</td>\n",
       "      <td>466</td>\n",
       "      <td>481</td>\n",
       "      <td>402</td>\n",
       "      <td>478</td>\n",
       "      <td>487</td>\n",
       "      <td>...</td>\n",
       "      <td>479</td>\n",
       "      <td>452</td>\n",
       "      <td>435</td>\n",
       "      <td>486</td>\n",
       "      <td>508</td>\n",
       "      <td>481</td>\n",
       "      <td>504</td>\n",
       "      <td>495</td>\n",
       "      <td>511</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 502 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  feat_000  feat_001  feat_002  feat_003  feat_004  feat_005  \\\n",
       "0      0       485       477       537       479       452       471   \n",
       "1      1       483       458       460       487       587       475   \n",
       "2      2       487       542       499       468       448       471   \n",
       "3      3       480       491       510       485       495       472   \n",
       "4      4       484       502       528       489       466       481   \n",
       "\n",
       "   feat_006  feat_007  feat_008  ...    feat_491  feat_492  feat_493  \\\n",
       "0       491       476       475  ...         481       477       485   \n",
       "1       526       479       485  ...         478       487       338   \n",
       "2       442       478       480  ...         481       492       650   \n",
       "3       417       474       502  ...         480       474       572   \n",
       "4       402       478       487  ...         479       452       435   \n",
       "\n",
       "   feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  label  \n",
       "0       511       485       481       479       475       496     -1  \n",
       "1       513       486       483       492       510       517     -1  \n",
       "2       506       501       480       489       499       498     -1  \n",
       "3       454       469       475       482       494       461      1  \n",
       "4       486       508       481       504       495       511      1  \n",
       "\n",
       "[5 rows x 502 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "madelon_feat_df= load_data_from_database()\n",
    "madelon_feat_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=madelon_feat_df.drop(['index', 'feat_003'], axis=1)\n",
    "y=madelon_feat_df['feat_003']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test':       feat_000  feat_001  feat_002  feat_004  feat_005  feat_006  feat_007  \\\n",
       " 1102       482       433       496       417       477       461       475   \n",
       " 772        474       529       533       533       490       543       474   \n",
       " 304        482       478       505       482       469       455       479   \n",
       " 511        468       473       516       514       473       512       476   \n",
       " 1289       491       498       542       516       477       435       476   \n",
       " 1882       482       530       514       549       482       391       478   \n",
       " 797        477       430       537       538       493       426       475   \n",
       " 971        481       438       501       508       461       481       476   \n",
       " 725        490       500       462       437       482       419       475   \n",
       " 484        480       475       490       493       492       496       477   \n",
       " 434        469       549       595       530       485       526       475   \n",
       " 727        480       430       504       520       487       398       477   \n",
       " 1432       488       408       521       549       480       488       476   \n",
       " 562        468       488       475       456       478       479       478   \n",
       " 638        480       468       522       509       475       397       476   \n",
       " 1629       478       483       491       539       479       499       477   \n",
       " 396        467       515       562       436       481       487       475   \n",
       " 859        479       400       507       528       487       451       476   \n",
       " 900        485       488       459       533       477       531       478   \n",
       " 1804       486       491       459       443       477       505       479   \n",
       " 515        486       508       481       520       490       547       478   \n",
       " 154        472       439       511       446       479       531       478   \n",
       " 1362       479       518       425       535       480       533       476   \n",
       " 730        487       501       498       455       476       392       478   \n",
       " 1078       468       427       497       423       473       468       478   \n",
       " 1345       489       422       605       492       486       435       478   \n",
       " 1015       487       503       466       407       472       493       478   \n",
       " 892        488       451       491       548       479       468       477   \n",
       " 1574       476       454       524       518       479       448       476   \n",
       " 199        490       484       516       492       473       599       478   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 1985       483       483       523       422       485       466       476   \n",
       " 371        479       482       483       533       477       458       478   \n",
       " 1696       477       417       462       495       481       493       477   \n",
       " 1048       462       434       529       526       477       488       476   \n",
       " 1201       480       495       489       500       490       483       476   \n",
       " 1561       486       496       482       490       479       490       476   \n",
       " 172        480       479       528       452       486       469       477   \n",
       " 1016       485       461       529       451       484       518       478   \n",
       " 1000       482       538       473       576       479       489       477   \n",
       " 570        484       434       502       526       475       527       478   \n",
       " 1315       480       444       514       536       473       470       477   \n",
       " 706        483       499       520       495       484       485       477   \n",
       " 917        479       433       411       478       479       482       478   \n",
       " 224        478       491       499       474       485       509       477   \n",
       " 6          484       533       498       577       482       471       477   \n",
       " 1018       481       485       551       556       493       573       477   \n",
       " 633        478       457       506       516       487       487       476   \n",
       " 1314       484       530       484       489       481       468       476   \n",
       " 1273       480       498       447       505       481       480       476   \n",
       " 1267       491       487       507       518       478       520       478   \n",
       " 868        482       527       551       580       480       497       475   \n",
       " 345        495       468       507       541       489       454       478   \n",
       " 87         480       464       483       454       498       479       477   \n",
       " 341        474       459       508       553       486       495       477   \n",
       " 1236       479       471       482       620       483       423       478   \n",
       " 1113       469       483       529       460       476       424       475   \n",
       " 1227       468       411       540       513       465       423       475   \n",
       " 1793       473       512       485       465       473       447       479   \n",
       " 1321       473       462       558       489       481       443       476   \n",
       " 848        478       456       555       480       475       478       478   \n",
       " \n",
       "       feat_008  feat_009  feat_010  ...    feat_491  feat_492  feat_493  \\\n",
       " 1102       516       471       499  ...         482       484       546   \n",
       " 772        459       471       450  ...         481       476       649   \n",
       " 304        502       477       450  ...         477       471       342   \n",
       " 511        481       471       483  ...         476       484       662   \n",
       " 1289       471       479       497  ...         474       467       643   \n",
       " 1882       493       482       500  ...         477       512       600   \n",
       " 797        492       467       473  ...         481       482       439   \n",
       " 971        481       499       453  ...         481       475       660   \n",
       " 725        505       472       462  ...         484       506       390   \n",
       " 484        494       473       502  ...         478       491       613   \n",
       " 434        468       478       505  ...         475       442       581   \n",
       " 727        506       471       419  ...         473       391       498   \n",
       " 1432       474       478       497  ...         481       481       395   \n",
       " 562        486       483       473  ...         480       497       520   \n",
       " 638        493       483       451  ...         484       477       634   \n",
       " 1629       470       474       473  ...         482       481       398   \n",
       " 396        491       469       538  ...         487       474       674   \n",
       " 859        472       494       500  ...         484       510       457   \n",
       " 900        483       467       512  ...         479       468       492   \n",
       " 1804       473       486       535  ...         479       481       552   \n",
       " 515        491       485       463  ...         478       475       401   \n",
       " 154        496       478       490  ...         479       496       665   \n",
       " 1362       507       483       484  ...         476       502       558   \n",
       " 730        469       472       529  ...         477       510       562   \n",
       " 1078       491       480       523  ...         477       517       527   \n",
       " 1345       461       483       482  ...         480       476       443   \n",
       " 1015       492       469       546  ...         483       476       483   \n",
       " 892        489       489       528  ...         478       494       306   \n",
       " 1574       491       484       514  ...         473       481       489   \n",
       " 199        467       477       454  ...         480       521       445   \n",
       " ...        ...       ...       ...  ...         ...       ...       ...   \n",
       " 1985       489       487       495  ...         475       445       600   \n",
       " 371        499       477       479  ...         471       459       337   \n",
       " 1696       489       475       505  ...         482       467       507   \n",
       " 1048       483       482       459  ...         479       485       675   \n",
       " 1201       486       484       480  ...         481       461       412   \n",
       " 1561       478       473       468  ...         479       528       394   \n",
       " 172        502       474       443  ...         480       506       283   \n",
       " 1016       472       468       516  ...         482       470       428   \n",
       " 1000       473       479       495  ...         480       482       382   \n",
       " 570        498       481       548  ...         486       523       602   \n",
       " 1315       486       486       497  ...         482       445       657   \n",
       " 706        488       491       510  ...         481       484       451   \n",
       " 917        485       472       422  ...         481       481       615   \n",
       " 224        461       475       484  ...         477       484       461   \n",
       " 6          476       468       476  ...         483       490       648   \n",
       " 1018       472       485       456  ...         476       467       306   \n",
       " 633        494       466       443  ...         486       520       381   \n",
       " 1314       460       484       559  ...         480       489       534   \n",
       " 1273       489       484       491  ...         474       493       598   \n",
       " 1267       503       467       372  ...         477       458       502   \n",
       " 868        496       473       520  ...         482       504       433   \n",
       " 345        481       487       354  ...         484       524       328   \n",
       " 87         513       477       516  ...         480       487       530   \n",
       " 341        484       484       540  ...         479       487       636   \n",
       " 1236       492       474       415  ...         482       533       680   \n",
       " 1113       477       485       471  ...         487       502       461   \n",
       " 1227       473       479       416  ...         481       489       435   \n",
       " 1793       489       465       567  ...         480       500       349   \n",
       " 1321       487       486       512  ...         484       443       611   \n",
       " 848        465       471       443  ...         479       423       550   \n",
       " \n",
       "       feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  label  \n",
       " 1102       500       467       473       486       549       534     -1  \n",
       " 772        537       603       476       474       547       506     -1  \n",
       " 304        516       498       476       490       517       470      1  \n",
       " 511        504       461       475       466       578       481     -1  \n",
       " 1289       527       545       486       484       519       479     -1  \n",
       " 1882       421       492       469       483       595       491      1  \n",
       " 797        527       495       478       470       472       409     -1  \n",
       " 971        455       518       482       483       459       474     -1  \n",
       " 725        494       499       483       470       441       488      1  \n",
       " 484        516       558       481       466       539       486     -1  \n",
       " 434        538       492       473       475       550       452      1  \n",
       " 727        457       573       481       507       508       461      1  \n",
       " 1432       482       487       485       483       551       482      1  \n",
       " 562        466       533       482       493       465       499     -1  \n",
       " 638        505       558       472       499       540       510     -1  \n",
       " 1629       505       557       482       482       533       444     -1  \n",
       " 396        462       512       463       474       487       510     -1  \n",
       " 859        513       547       473       474       487       475      1  \n",
       " 900        456       504       476       453       512       495      1  \n",
       " 1804       469       483       483       484       533       454      1  \n",
       " 515        455       537       482       473       454       497     -1  \n",
       " 154        543       547       475       485       472       523     -1  \n",
       " 1362       456       525       482       473       550       498      1  \n",
       " 730        502       549       473       478       444       497      1  \n",
       " 1078       498       487       479       499       467       487     -1  \n",
       " 1345       492       523       479       484       521       455     -1  \n",
       " 1015       523       494       470       488       488       525     -1  \n",
       " 892        498       547       478       476       483       473     -1  \n",
       " 1574       474       526       480       490       486       481     -1  \n",
       " 199        483       574       484       488       519       501      1  \n",
       " ...        ...       ...       ...       ...       ...       ...    ...  \n",
       " 1985       488       515       479       470       500       451      1  \n",
       " 371        481       530       475       472       491       504     -1  \n",
       " 1696       489       519       476       490       522       477     -1  \n",
       " 1048       418       441       480       479       510       483     -1  \n",
       " 1201       511       530       481       491       525       516      1  \n",
       " 1561       456       571       470       495       454       465     -1  \n",
       " 172        449       448       473       473       496       492      1  \n",
       " 1016       539       492       485       510       500       513     -1  \n",
       " 1000       522       486       475       497       487       439     -1  \n",
       " 570        537       510       481       478       520       503     -1  \n",
       " 1315       481       557       478       484       468       490      1  \n",
       " 706        445       443       481       485       492       477      1  \n",
       " 917        485       559       472       478       508       466      1  \n",
       " 224        477       537       466       459       520       524     -1  \n",
       " 6          484       475       479       475       462       484      1  \n",
       " 1018       481       499       476       486       518       443     -1  \n",
       " 633        538       498       486       487       485       463     -1  \n",
       " 1314       498       543       475       490       530       448     -1  \n",
       " 1273       496       491       477       512       559       518      1  \n",
       " 1267       513       564       480       487       488       496     -1  \n",
       " 868        474       457       469       478       523       520     -1  \n",
       " 345        512       529       471       484       482       456     -1  \n",
       " 87         489       494       479       482       462       477     -1  \n",
       " 341        515       478       479       507       491       508      1  \n",
       " 1236       540       466       475       475       494       514      1  \n",
       " 1113       488       522       479       474       491       481     -1  \n",
       " 1227       542       481       473       505       486       472      1  \n",
       " 1793       493       551       470       484       492       472      1  \n",
       " 1321       510       494       483       499       543       480      1  \n",
       " 848        455       519       485       504       494       440     -1  \n",
       " \n",
       " [500 rows x 500 columns],\n",
       " 'X_train':       feat_000  feat_001  feat_002  feat_004  feat_005  feat_006  feat_007  \\\n",
       " 804        484       501       537       498       484       481       476   \n",
       " 1664       471       467       506       471       479       480       475   \n",
       " 1372       471       488       486       464       484       401       475   \n",
       " 812        473       470       515       434       478       444       476   \n",
       " 1812       494       460       521       483       476       524       477   \n",
       " 1156       464       523       508       486       486       488       478   \n",
       " 130        489       534       396       537       481       531       477   \n",
       " 930        480       505       494       470       463       511       479   \n",
       " 1075       472       491       512       533       480       478       478   \n",
       " 1183       484       445       515       513       481       426       478   \n",
       " 160        479       489       539       528       489       425       477   \n",
       " 507        478       496       488       494       494       518       477   \n",
       " 1550       474       482       422       494       477       493       478   \n",
       " 964        482       531       486       475       481       468       477   \n",
       " 1340       492       462       533       561       471       499       477   \n",
       " 1934       482       535       541       574       474       522       476   \n",
       " 1137       481       463       498       520       472       536       477   \n",
       " 424        489       476       508       461       485       507       478   \n",
       " 1518       464       467       490       496       480       555       479   \n",
       " 360        498       471       472       480       477       445       477   \n",
       " 485        485       487       524       513       477       402       475   \n",
       " 207        493       520       463       552       491       530       474   \n",
       " 1560       469       466       485       469       465       508       475   \n",
       " 1790       480       455       516       468       486       447       475   \n",
       " 1380       472       395       507       549       476       480       476   \n",
       " 42         475       511       466       508       473       471       477   \n",
       " 1719       480       483       478       507       477       529       476   \n",
       " 148        480       522       543       471       481       452       476   \n",
       " 166        476       474       507       526       479       545       475   \n",
       " 644        494       503       526       444       475       502       475   \n",
       " ...        ...       ...       ...       ...       ...       ...       ...   \n",
       " 1842       476       518       548       499       485       498       477   \n",
       " 1127       477       544       461       467       483       429       477   \n",
       " 920        483       407       473       434       475       566       476   \n",
       " 1903       474       476       472       535       477       533       478   \n",
       " 59         488       507       486       575       482       493       475   \n",
       " 742        483       466       509       432       472       439       477   \n",
       " 1810       487       484       603       514       478       514       477   \n",
       " 1580       473       487       502       491       484       449       475   \n",
       " 1374       470       496       443       511       463       372       477   \n",
       " 1277       476       525       536       512       487       468       475   \n",
       " 1597       464       445       557       456       477       449       476   \n",
       " 582        484       491       572       495       472       504       478   \n",
       " 1198       482       499       561       471       479       396       477   \n",
       " 774        488       474       532       518       482       528       476   \n",
       " 457        493       499       534       489       480       530       479   \n",
       " 90         473       447       510       464       477       471       476   \n",
       " 495        493       465       484       505       475       430       477   \n",
       " 1539       480       454       452       521       482       453       477   \n",
       " 968        494       476       461       506       480       463       477   \n",
       " 1528       473       479       487       462       470       490       477   \n",
       " 335        482       519       487       574       480       479       478   \n",
       " 1248       482       481       472       425       473       564       477   \n",
       " 11         486       479       528       530       484       479       477   \n",
       " 1409       488       480       524       547       479       443       475   \n",
       " 1041       472       484       460       515       481       463       477   \n",
       " 1663       497       480       496       471       505       513       477   \n",
       " 1384       483       511       480       449       484       467       479   \n",
       " 1142       485       541       455       516       475       465       477   \n",
       " 808        478       524       540       523       491       501       475   \n",
       " 814        487       487       551       486       470       485       476   \n",
       " \n",
       "       feat_008  feat_009  feat_010  ...    feat_491  feat_492  feat_493  \\\n",
       " 804        515       487       455  ...         482       528       660   \n",
       " 1664       483       468       432  ...         484       483       402   \n",
       " 1372       522       483       511  ...         481       486       693   \n",
       " 812        486       484       483  ...         480       502       543   \n",
       " 1812       490       475       451  ...         474       489       487   \n",
       " 1156       486       468       503  ...         474       503       518   \n",
       " 130        487       482       443  ...         480       536       528   \n",
       " 930        490       480       500  ...         476       514       548   \n",
       " 1075       463       489       474  ...         487       497       564   \n",
       " 1183       491       479       468  ...         479       475       451   \n",
       " 160        505       474       543  ...         476       499       453   \n",
       " 507        462       481       535  ...         486       482       376   \n",
       " 1550       477       471       524  ...         477       483       471   \n",
       " 964        467       481       499  ...         475       515       533   \n",
       " 1340       493       474       447  ...         483       463       760   \n",
       " 1934       453       486       467  ...         480       486       593   \n",
       " 1137       492       485       545  ...         479       479       275   \n",
       " 424        487       480       523  ...         478       483       434   \n",
       " 1518       479       486       479  ...         477       496       596   \n",
       " 360        473       477       484  ...         477       465       567   \n",
       " 485        499       478       466  ...         477       491       553   \n",
       " 207        476       473       463  ...         480       438       703   \n",
       " 1560       462       477       521  ...         471       453       516   \n",
       " 1790       518       481       516  ...         481       482       481   \n",
       " 1380       481       487       449  ...         476       487       563   \n",
       " 42         496       469       513  ...         476       471       537   \n",
       " 1719       502       472       396  ...         485       458       240   \n",
       " 148        503       478       485  ...         478       474       529   \n",
       " 166        501       478       426  ...         481       471       490   \n",
       " 644        495       475       507  ...         474       474       545   \n",
       " ...        ...       ...       ...  ...         ...       ...       ...   \n",
       " 1842       494       479       451  ...         481       478       655   \n",
       " 1127       491       486       514  ...         482       470       578   \n",
       " 920        492       492       443  ...         473       453       379   \n",
       " 1903       462       476       491  ...         476       475       720   \n",
       " 59         457       470       464  ...         473       468       440   \n",
       " 742        502       484       491  ...         482       471       420   \n",
       " 1810       516       492       513  ...         480       546       442   \n",
       " 1580       487       474       521  ...         487       540       621   \n",
       " 1374       457       487       481  ...         474       498       330   \n",
       " 1277       491       482       472  ...         480       457       473   \n",
       " 1597       484       469       475  ...         470       501       542   \n",
       " 582        496       483       476  ...         477       516       553   \n",
       " 1198       478       477       464  ...         470       510       547   \n",
       " 774        474       485       432  ...         475       485       266   \n",
       " 457        487       473       455  ...         484       496       383   \n",
       " 90         495       477       567  ...         481       498       541   \n",
       " 495        517       473       469  ...         481       507       456   \n",
       " 1539       483       481       554  ...         479       484       470   \n",
       " 968        509       483       442  ...         471       532       274   \n",
       " 1528       510       478       580  ...         482       540       294   \n",
       " 335        494       468       548  ...         479       474       329   \n",
       " 1248       482       496       471  ...         476       462       533   \n",
       " 11         462       487       499  ...         476       498       599   \n",
       " 1409       499       488       528  ...         476       482       554   \n",
       " 1041       476       478       492  ...         479       497       461   \n",
       " 1663       494       483       454  ...         481       536       357   \n",
       " 1384       493       479       486  ...         476       485       486   \n",
       " 1142       477       482       449  ...         484       528       509   \n",
       " 808        476       464       486  ...         469       466       546   \n",
       " 814        475       491       490  ...         482       489       350   \n",
       " \n",
       "       feat_494  feat_495  feat_496  feat_497  feat_498  feat_499  label  \n",
       " 804        510       583       471       478       486       534     -1  \n",
       " 1664       451       507       476       481       546       476     -1  \n",
       " 1372       513       523       480       478       546       524     -1  \n",
       " 812        510       459       477       482       563       476     -1  \n",
       " 1812       575       489       478       452       535       524     -1  \n",
       " 1156       541       464       473       483       517       474      1  \n",
       " 130        487       522       478       461       526       440     -1  \n",
       " 930        468       446       486       471       494       522     -1  \n",
       " 1075       539       544       483       488       490       474      1  \n",
       " 1183       520       526       486       497       586       463      1  \n",
       " 160        499       467       487       485       465       502     -1  \n",
       " 507        514       522       481       496       536       524     -1  \n",
       " 1550       415       516       483       501       451       509     -1  \n",
       " 964        511       496       473       480       506       516     -1  \n",
       " 1340       554       509       476       468       481       491     -1  \n",
       " 1934       511       507       480       525       539       532      1  \n",
       " 1137       453       470       485       483       532       516     -1  \n",
       " 424        613       501       484       454       548       491     -1  \n",
       " 1518       399       464       484       487       559       483     -1  \n",
       " 360        407       547       481       464       387       511      1  \n",
       " 485        445       532       486       484       545       498      1  \n",
       " 207        470       503       486       495       419       506     -1  \n",
       " 1560       475       540       472       493       486       494     -1  \n",
       " 1790       510       503       484       469       510       513     -1  \n",
       " 1380       475       465       484       488       548       515     -1  \n",
       " 42         484       417       477       492       537       487      1  \n",
       " 1719       542       431       482       469       522       510      1  \n",
       " 148        539       548       473       479       515       465     -1  \n",
       " 166        508       573       476       497       522       477      1  \n",
       " 644        486       535       482       481       552       473     -1  \n",
       " ...        ...       ...       ...       ...       ...       ...    ...  \n",
       " 1842       443       440       475       487       514       517      1  \n",
       " 1127       498       558       478       485       515       521      1  \n",
       " 920        456       566       481       492       478       507     -1  \n",
       " 1903       560       501       484       512       434       475      1  \n",
       " 59         498       542       479       486       505       458      1  \n",
       " 742        505       524       476       451       560       473     -1  \n",
       " 1810       501       533       468       477       466       536      1  \n",
       " 1580       481       526       467       508       482       458      1  \n",
       " 1374       509       526       487       502       486       493      1  \n",
       " 1277       528       475       481       512       468       517     -1  \n",
       " 1597       491       489       477       487       422       465      1  \n",
       " 582        505       547       485       481       558       470      1  \n",
       " 1198       468       475       472       494       501       511      1  \n",
       " 774        598       547       479       490       550       524      1  \n",
       " 457        477       486       491       472       549       499      1  \n",
       " 90         496       531       471       453       531       454     -1  \n",
       " 495        507       491       482       480       523       496     -1  \n",
       " 1539       484       477       483       500       478       515      1  \n",
       " 968        486       502       489       487       504       461      1  \n",
       " 1528       490       489       470       487       519       505      1  \n",
       " 335        554       457       478       468       514       518      1  \n",
       " 1248       533       483       473       458       496       441     -1  \n",
       " 11         459       452       475       496       505       485     -1  \n",
       " 1409       561       551       481       471       531       490      1  \n",
       " 1041       527       545       476       471       530       530      1  \n",
       " 1663       436       535       483       473       601       502     -1  \n",
       " 1384       478       506       480       501       457       515     -1  \n",
       " 1142       506       518       479       498       465       501     -1  \n",
       " 808        442       525       485       493       500       496      1  \n",
       " 814        530       567       483       496       466       501      1  \n",
       " \n",
       " [1500 rows x 500 columns],\n",
       " 'y_test': 1102    496\n",
       " 772     486\n",
       " 304     489\n",
       " 511     474\n",
       " 1289    475\n",
       " 1882    489\n",
       " 797     481\n",
       " 971     498\n",
       " 725     482\n",
       " 484     508\n",
       " 434     476\n",
       " 727     493\n",
       " 1432    477\n",
       " 562     488\n",
       " 638     498\n",
       " 1629    499\n",
       " 396     495\n",
       " 859     472\n",
       " 900     481\n",
       " 1804    486\n",
       " 515     480\n",
       " 154     478\n",
       " 1362    468\n",
       " 730     494\n",
       " 1078    495\n",
       " 1345    463\n",
       " 1015    505\n",
       " 892     475\n",
       " 1574    478\n",
       " 199     479\n",
       "        ... \n",
       " 1985    489\n",
       " 371     491\n",
       " 1696    497\n",
       " 1048    473\n",
       " 1201    497\n",
       " 1561    463\n",
       " 172     477\n",
       " 1016    473\n",
       " 1000    485\n",
       " 570     474\n",
       " 1315    489\n",
       " 706     467\n",
       " 917     483\n",
       " 224     477\n",
       " 6       466\n",
       " 1018    479\n",
       " 633     484\n",
       " 1314    489\n",
       " 1273    491\n",
       " 1267    489\n",
       " 868     493\n",
       " 345     498\n",
       " 87      471\n",
       " 341     484\n",
       " 1236    492\n",
       " 1113    484\n",
       " 1227    475\n",
       " 1793    479\n",
       " 1321    480\n",
       " 848     488\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 804     484\n",
       " 1664    481\n",
       " 1372    460\n",
       " 812     482\n",
       " 1812    482\n",
       " 1156    467\n",
       " 130     499\n",
       " 930     471\n",
       " 1075    478\n",
       " 1183    474\n",
       " 160     490\n",
       " 507     483\n",
       " 1550    477\n",
       " 964     502\n",
       " 1340    479\n",
       " 1934    475\n",
       " 1137    477\n",
       " 424     481\n",
       " 1518    469\n",
       " 360     484\n",
       " 485     463\n",
       " 207     482\n",
       " 1560    482\n",
       " 1790    490\n",
       " 1380    479\n",
       " 42      484\n",
       " 1719    478\n",
       " 148     498\n",
       " 166     493\n",
       " 644     471\n",
       "        ... \n",
       " 1842    482\n",
       " 1127    484\n",
       " 920     480\n",
       " 1903    473\n",
       " 59      500\n",
       " 742     483\n",
       " 1810    476\n",
       " 1580    479\n",
       " 1374    481\n",
       " 1277    502\n",
       " 1597    495\n",
       " 582     479\n",
       " 1198    488\n",
       " 774     476\n",
       " 457     491\n",
       " 90      479\n",
       " 495     497\n",
       " 1539    480\n",
       " 968     463\n",
       " 1528    470\n",
       " 335     487\n",
       " 1248    487\n",
       " 11      483\n",
       " 1409    479\n",
       " 1041    465\n",
       " 1663    481\n",
       " 1384    478\n",
       " 1142    481\n",
       " 808     453\n",
       " 814     490\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_data_dict(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dict=make_data_dict(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "         -0.23528695, -0.99733688],\n",
       "        [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "          0.53136977, -0.99733688],\n",
       "        [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "         -0.58028247,  1.00267023],\n",
       "        ..., \n",
       "        [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "         -1.15527501, -0.99733688],\n",
       "        [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "         -0.04362277, -0.99733688],\n",
       "        [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "          2.56301009, -0.99733688]]),\n",
       " 'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "         -0.61861531,  1.00267023],\n",
       "        [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "         -0.38861829,  1.00267023],\n",
       "        [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "          0.83803246,  1.00267023],\n",
       "        ..., \n",
       "        [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "          0.56970261, -0.99733688],\n",
       "        [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "          0.26303992,  1.00267023],\n",
       "        [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "         -0.65694814, -0.99733688]]),\n",
       " 'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "          -0.23528695, -0.99733688],\n",
       "         [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "           0.53136977, -0.99733688],\n",
       "         [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "          -0.58028247,  1.00267023],\n",
       "         ..., \n",
       "         [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "          -1.15527501, -0.99733688],\n",
       "         [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "          -0.04362277, -0.99733688],\n",
       "         [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "           2.56301009, -0.99733688]]),\n",
       "  'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "          -0.61861531,  1.00267023],\n",
       "         [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "          -0.38861829,  1.00267023],\n",
       "         [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "           0.83803246,  1.00267023],\n",
       "         ..., \n",
       "         [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "           0.56970261, -0.99733688],\n",
       "         [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "           0.26303992,  1.00267023],\n",
       "         [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "          -0.65694814, -0.99733688]]),\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "general_transformer(StandardScaler(),data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SS_data_dict=general_transformer(StandardScaler(),data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "        [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "        [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "        ..., \n",
       "        [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "        [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "        [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       " 'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "        [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "        [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "        ..., \n",
       "        [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "        [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "        [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       " 'data_dict': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "         [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "         [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "         ..., \n",
       "         [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "         [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "         [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "  'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "         [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "         [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "         ..., \n",
       "         [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "         [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "         [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "  'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "           -0.23528695, -0.99733688],\n",
       "          [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "            0.53136977, -0.99733688],\n",
       "          [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "           -0.58028247,  1.00267023],\n",
       "          ..., \n",
       "          [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "           -1.15527501, -0.99733688],\n",
       "          [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "           -0.04362277, -0.99733688],\n",
       "          [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "            2.56301009, -0.99733688]]),\n",
       "   'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "           -0.61861531,  1.00267023],\n",
       "          [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "           -0.38861829,  1.00267023],\n",
       "          [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "            0.83803246,  1.00267023],\n",
       "          ..., \n",
       "          [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "            0.56970261, -0.99733688],\n",
       "          [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "            0.26303992,  1.00267023],\n",
       "          [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "           -0.65694814, -0.99733688]]),\n",
       "   'y_test': 1353    482\n",
       "   558     476\n",
       "   1566    486\n",
       "   76      488\n",
       "   980     477\n",
       "   1227    475\n",
       "   1466    470\n",
       "   606     492\n",
       "   682     472\n",
       "   460     492\n",
       "   1230    487\n",
       "   247     482\n",
       "   1322    483\n",
       "   1161    487\n",
       "   238     489\n",
       "   1719    478\n",
       "   417     462\n",
       "   186     482\n",
       "   557     475\n",
       "   1563    465\n",
       "   1391    475\n",
       "   796     488\n",
       "   294     485\n",
       "   1536    494\n",
       "   661     488\n",
       "   1547    476\n",
       "   203     486\n",
       "   1045    476\n",
       "   292     490\n",
       "   1272    476\n",
       "          ... \n",
       "   1687    474\n",
       "   1661    490\n",
       "   1690    480\n",
       "   394     488\n",
       "   1562    481\n",
       "   1457    482\n",
       "   1052    502\n",
       "   59      500\n",
       "   1725    486\n",
       "   793     475\n",
       "   1879    485\n",
       "   823     471\n",
       "   638     498\n",
       "   1450    492\n",
       "   1438    476\n",
       "   1791    492\n",
       "   1912    476\n",
       "   504     490\n",
       "   1948    494\n",
       "   1025    479\n",
       "   1298    482\n",
       "   60      490\n",
       "   268     479\n",
       "   471     488\n",
       "   1116    493\n",
       "   1144    490\n",
       "   37      479\n",
       "   234     490\n",
       "   1222    470\n",
       "   749     476\n",
       "   Name: feat_003, dtype: int64,\n",
       "   'y_train': 1156    467\n",
       "   1139    479\n",
       "   1945    498\n",
       "   1004    478\n",
       "   185     482\n",
       "   1145    490\n",
       "   1623    472\n",
       "   300     474\n",
       "   576     493\n",
       "   518     474\n",
       "   1479    486\n",
       "   1927    491\n",
       "   218     497\n",
       "   554     492\n",
       "   1017    502\n",
       "   1884    477\n",
       "   23      479\n",
       "   472     491\n",
       "   542     496\n",
       "   1852    495\n",
       "   950     477\n",
       "   44      487\n",
       "   1840    464\n",
       "   506     479\n",
       "   800     472\n",
       "   475     495\n",
       "   1574    478\n",
       "   139     483\n",
       "   198     486\n",
       "   132     487\n",
       "          ... \n",
       "   891     490\n",
       "   1656    476\n",
       "   600     487\n",
       "   1140    496\n",
       "   528     475\n",
       "   121     470\n",
       "   840     478\n",
       "   1023    482\n",
       "   1240    482\n",
       "   17      478\n",
       "   222     483\n",
       "   267     485\n",
       "   408     472\n",
       "   1977    480\n",
       "   1275    482\n",
       "   525     481\n",
       "   991     467\n",
       "   896     474\n",
       "   982     491\n",
       "   1092    475\n",
       "   535     486\n",
       "   1604    485\n",
       "   1201    497\n",
       "   1843    488\n",
       "   161     478\n",
       "   1526    483\n",
       "   603     493\n",
       "   1522    493\n",
       "   1068    481\n",
       "   1841    486\n",
       "   Name: feat_003, dtype: int64},\n",
       "  'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'transformer': SelectKBest(k=5, score_func=<function f_classif at 0x114742a28>),\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "general_transformer(SelectKBest(k=5),SS_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KB_data_dict=general_transformer(SelectKBest(k=5),SS_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "        [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "        [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "        ..., \n",
       "        [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "        [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "        [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       " 'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "        [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "        [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "        ..., \n",
       "        [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "        [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "        [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       " 'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "         [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "         [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "         ..., \n",
       "         [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "         [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "         [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "  'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "         [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "         [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "         ..., \n",
       "         [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "         [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "         [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "  'data_dict': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "          [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "          [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "          ..., \n",
       "          [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "          [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "          [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "   'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "          [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "          [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "          ..., \n",
       "          [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "          [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "          [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "   'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "            -0.23528695, -0.99733688],\n",
       "           [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "             0.53136977, -0.99733688],\n",
       "           [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "            -0.58028247,  1.00267023],\n",
       "           ..., \n",
       "           [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "            -1.15527501, -0.99733688],\n",
       "           [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "            -0.04362277, -0.99733688],\n",
       "           [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "             2.56301009, -0.99733688]]),\n",
       "    'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "            -0.61861531,  1.00267023],\n",
       "           [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "            -0.38861829,  1.00267023],\n",
       "           [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "             0.83803246,  1.00267023],\n",
       "           ..., \n",
       "           [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "             0.56970261, -0.99733688],\n",
       "           [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "             0.26303992,  1.00267023],\n",
       "           [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "            -0.65694814, -0.99733688]]),\n",
       "    'y_test': 1353    482\n",
       "    558     476\n",
       "    1566    486\n",
       "    76      488\n",
       "    980     477\n",
       "    1227    475\n",
       "    1466    470\n",
       "    606     492\n",
       "    682     472\n",
       "    460     492\n",
       "    1230    487\n",
       "    247     482\n",
       "    1322    483\n",
       "    1161    487\n",
       "    238     489\n",
       "    1719    478\n",
       "    417     462\n",
       "    186     482\n",
       "    557     475\n",
       "    1563    465\n",
       "    1391    475\n",
       "    796     488\n",
       "    294     485\n",
       "    1536    494\n",
       "    661     488\n",
       "    1547    476\n",
       "    203     486\n",
       "    1045    476\n",
       "    292     490\n",
       "    1272    476\n",
       "           ... \n",
       "    1687    474\n",
       "    1661    490\n",
       "    1690    480\n",
       "    394     488\n",
       "    1562    481\n",
       "    1457    482\n",
       "    1052    502\n",
       "    59      500\n",
       "    1725    486\n",
       "    793     475\n",
       "    1879    485\n",
       "    823     471\n",
       "    638     498\n",
       "    1450    492\n",
       "    1438    476\n",
       "    1791    492\n",
       "    1912    476\n",
       "    504     490\n",
       "    1948    494\n",
       "    1025    479\n",
       "    1298    482\n",
       "    60      490\n",
       "    268     479\n",
       "    471     488\n",
       "    1116    493\n",
       "    1144    490\n",
       "    37      479\n",
       "    234     490\n",
       "    1222    470\n",
       "    749     476\n",
       "    Name: feat_003, dtype: int64,\n",
       "    'y_train': 1156    467\n",
       "    1139    479\n",
       "    1945    498\n",
       "    1004    478\n",
       "    185     482\n",
       "    1145    490\n",
       "    1623    472\n",
       "    300     474\n",
       "    576     493\n",
       "    518     474\n",
       "    1479    486\n",
       "    1927    491\n",
       "    218     497\n",
       "    554     492\n",
       "    1017    502\n",
       "    1884    477\n",
       "    23      479\n",
       "    472     491\n",
       "    542     496\n",
       "    1852    495\n",
       "    950     477\n",
       "    44      487\n",
       "    1840    464\n",
       "    506     479\n",
       "    800     472\n",
       "    475     495\n",
       "    1574    478\n",
       "    139     483\n",
       "    198     486\n",
       "    132     487\n",
       "           ... \n",
       "    891     490\n",
       "    1656    476\n",
       "    600     487\n",
       "    1140    496\n",
       "    528     475\n",
       "    121     470\n",
       "    840     478\n",
       "    1023    482\n",
       "    1240    482\n",
       "    17      478\n",
       "    222     483\n",
       "    267     485\n",
       "    408     472\n",
       "    1977    480\n",
       "    1275    482\n",
       "    525     481\n",
       "    991     467\n",
       "    896     474\n",
       "    982     491\n",
       "    1092    475\n",
       "    535     486\n",
       "    1604    485\n",
       "    1201    497\n",
       "    1843    488\n",
       "    161     478\n",
       "    1526    483\n",
       "    603     493\n",
       "    1522    493\n",
       "    1068    481\n",
       "    1841    486\n",
       "    Name: feat_003, dtype: int64},\n",
       "   'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   'y_test': 1353    482\n",
       "   558     476\n",
       "   1566    486\n",
       "   76      488\n",
       "   980     477\n",
       "   1227    475\n",
       "   1466    470\n",
       "   606     492\n",
       "   682     472\n",
       "   460     492\n",
       "   1230    487\n",
       "   247     482\n",
       "   1322    483\n",
       "   1161    487\n",
       "   238     489\n",
       "   1719    478\n",
       "   417     462\n",
       "   186     482\n",
       "   557     475\n",
       "   1563    465\n",
       "   1391    475\n",
       "   796     488\n",
       "   294     485\n",
       "   1536    494\n",
       "   661     488\n",
       "   1547    476\n",
       "   203     486\n",
       "   1045    476\n",
       "   292     490\n",
       "   1272    476\n",
       "          ... \n",
       "   1687    474\n",
       "   1661    490\n",
       "   1690    480\n",
       "   394     488\n",
       "   1562    481\n",
       "   1457    482\n",
       "   1052    502\n",
       "   59      500\n",
       "   1725    486\n",
       "   793     475\n",
       "   1879    485\n",
       "   823     471\n",
       "   638     498\n",
       "   1450    492\n",
       "   1438    476\n",
       "   1791    492\n",
       "   1912    476\n",
       "   504     490\n",
       "   1948    494\n",
       "   1025    479\n",
       "   1298    482\n",
       "   60      490\n",
       "   268     479\n",
       "   471     488\n",
       "   1116    493\n",
       "   1144    490\n",
       "   37      479\n",
       "   234     490\n",
       "   1222    470\n",
       "   749     476\n",
       "   Name: feat_003, dtype: int64,\n",
       "   'y_train': 1156    467\n",
       "   1139    479\n",
       "   1945    498\n",
       "   1004    478\n",
       "   185     482\n",
       "   1145    490\n",
       "   1623    472\n",
       "   300     474\n",
       "   576     493\n",
       "   518     474\n",
       "   1479    486\n",
       "   1927    491\n",
       "   218     497\n",
       "   554     492\n",
       "   1017    502\n",
       "   1884    477\n",
       "   23      479\n",
       "   472     491\n",
       "   542     496\n",
       "   1852    495\n",
       "   950     477\n",
       "   44      487\n",
       "   1840    464\n",
       "   506     479\n",
       "   800     472\n",
       "   475     495\n",
       "   1574    478\n",
       "   139     483\n",
       "   198     486\n",
       "   132     487\n",
       "          ... \n",
       "   891     490\n",
       "   1656    476\n",
       "   600     487\n",
       "   1140    496\n",
       "   528     475\n",
       "   121     470\n",
       "   840     478\n",
       "   1023    482\n",
       "   1240    482\n",
       "   17      478\n",
       "   222     483\n",
       "   267     485\n",
       "   408     472\n",
       "   1977    480\n",
       "   1275    482\n",
       "   525     481\n",
       "   991     467\n",
       "   896     474\n",
       "   982     491\n",
       "   1092    475\n",
       "   535     486\n",
       "   1604    485\n",
       "   1201    497\n",
       "   1843    488\n",
       "   161     478\n",
       "   1526    483\n",
       "   603     493\n",
       "   1522    493\n",
       "   1068    481\n",
       "   1841    486\n",
       "   Name: feat_003, dtype: int64},\n",
       "  'test_score': 0.035999999999999997,\n",
       "  'train_score': 0.075999999999999998,\n",
       "  'transformer': SelectKBest(k=5, score_func=<function f_classif at 0x114742a28>),\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False),\n",
       " 'test_score': 0.035999999999999997,\n",
       " 'train_score': 0.075999999999999998,\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "general_model(LogisticRegression(),KB_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LR_data_dict=general_model(LogisticRegression(),KB_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "        [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "        [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "        ..., \n",
       "        [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "        [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "        [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       " 'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "        [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "        [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "        ..., \n",
       "        [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "        [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "        [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       " 'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "         [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "         [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "         ..., \n",
       "         [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "         [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "         [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "  'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "         [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "         [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "         ..., \n",
       "         [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "         [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "         [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "  'data_dict': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "          [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "          [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "          ..., \n",
       "          [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "          [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "          [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "   'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "          [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "          [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "          ..., \n",
       "          [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "          [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "          [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "   'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "            -0.23528695, -0.99733688],\n",
       "           [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "             0.53136977, -0.99733688],\n",
       "           [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "            -0.58028247,  1.00267023],\n",
       "           ..., \n",
       "           [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "            -1.15527501, -0.99733688],\n",
       "           [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "            -0.04362277, -0.99733688],\n",
       "           [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "             2.56301009, -0.99733688]]),\n",
       "    'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "            -0.61861531,  1.00267023],\n",
       "           [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "            -0.38861829,  1.00267023],\n",
       "           [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "             0.83803246,  1.00267023],\n",
       "           ..., \n",
       "           [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "             0.56970261, -0.99733688],\n",
       "           [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "             0.26303992,  1.00267023],\n",
       "           [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "            -0.65694814, -0.99733688]]),\n",
       "    'y_test': 1353    482\n",
       "    558     476\n",
       "    1566    486\n",
       "    76      488\n",
       "    980     477\n",
       "    1227    475\n",
       "    1466    470\n",
       "    606     492\n",
       "    682     472\n",
       "    460     492\n",
       "    1230    487\n",
       "    247     482\n",
       "    1322    483\n",
       "    1161    487\n",
       "    238     489\n",
       "    1719    478\n",
       "    417     462\n",
       "    186     482\n",
       "    557     475\n",
       "    1563    465\n",
       "    1391    475\n",
       "    796     488\n",
       "    294     485\n",
       "    1536    494\n",
       "    661     488\n",
       "    1547    476\n",
       "    203     486\n",
       "    1045    476\n",
       "    292     490\n",
       "    1272    476\n",
       "           ... \n",
       "    1687    474\n",
       "    1661    490\n",
       "    1690    480\n",
       "    394     488\n",
       "    1562    481\n",
       "    1457    482\n",
       "    1052    502\n",
       "    59      500\n",
       "    1725    486\n",
       "    793     475\n",
       "    1879    485\n",
       "    823     471\n",
       "    638     498\n",
       "    1450    492\n",
       "    1438    476\n",
       "    1791    492\n",
       "    1912    476\n",
       "    504     490\n",
       "    1948    494\n",
       "    1025    479\n",
       "    1298    482\n",
       "    60      490\n",
       "    268     479\n",
       "    471     488\n",
       "    1116    493\n",
       "    1144    490\n",
       "    37      479\n",
       "    234     490\n",
       "    1222    470\n",
       "    749     476\n",
       "    Name: feat_003, dtype: int64,\n",
       "    'y_train': 1156    467\n",
       "    1139    479\n",
       "    1945    498\n",
       "    1004    478\n",
       "    185     482\n",
       "    1145    490\n",
       "    1623    472\n",
       "    300     474\n",
       "    576     493\n",
       "    518     474\n",
       "    1479    486\n",
       "    1927    491\n",
       "    218     497\n",
       "    554     492\n",
       "    1017    502\n",
       "    1884    477\n",
       "    23      479\n",
       "    472     491\n",
       "    542     496\n",
       "    1852    495\n",
       "    950     477\n",
       "    44      487\n",
       "    1840    464\n",
       "    506     479\n",
       "    800     472\n",
       "    475     495\n",
       "    1574    478\n",
       "    139     483\n",
       "    198     486\n",
       "    132     487\n",
       "           ... \n",
       "    891     490\n",
       "    1656    476\n",
       "    600     487\n",
       "    1140    496\n",
       "    528     475\n",
       "    121     470\n",
       "    840     478\n",
       "    1023    482\n",
       "    1240    482\n",
       "    17      478\n",
       "    222     483\n",
       "    267     485\n",
       "    408     472\n",
       "    1977    480\n",
       "    1275    482\n",
       "    525     481\n",
       "    991     467\n",
       "    896     474\n",
       "    982     491\n",
       "    1092    475\n",
       "    535     486\n",
       "    1604    485\n",
       "    1201    497\n",
       "    1843    488\n",
       "    161     478\n",
       "    1526    483\n",
       "    603     493\n",
       "    1522    493\n",
       "    1068    481\n",
       "    1841    486\n",
       "    Name: feat_003, dtype: int64},\n",
       "   'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "   'y_test': 1353    482\n",
       "   558     476\n",
       "   1566    486\n",
       "   76      488\n",
       "   980     477\n",
       "   1227    475\n",
       "   1466    470\n",
       "   606     492\n",
       "   682     472\n",
       "   460     492\n",
       "   1230    487\n",
       "   247     482\n",
       "   1322    483\n",
       "   1161    487\n",
       "   238     489\n",
       "   1719    478\n",
       "   417     462\n",
       "   186     482\n",
       "   557     475\n",
       "   1563    465\n",
       "   1391    475\n",
       "   796     488\n",
       "   294     485\n",
       "   1536    494\n",
       "   661     488\n",
       "   1547    476\n",
       "   203     486\n",
       "   1045    476\n",
       "   292     490\n",
       "   1272    476\n",
       "          ... \n",
       "   1687    474\n",
       "   1661    490\n",
       "   1690    480\n",
       "   394     488\n",
       "   1562    481\n",
       "   1457    482\n",
       "   1052    502\n",
       "   59      500\n",
       "   1725    486\n",
       "   793     475\n",
       "   1879    485\n",
       "   823     471\n",
       "   638     498\n",
       "   1450    492\n",
       "   1438    476\n",
       "   1791    492\n",
       "   1912    476\n",
       "   504     490\n",
       "   1948    494\n",
       "   1025    479\n",
       "   1298    482\n",
       "   60      490\n",
       "   268     479\n",
       "   471     488\n",
       "   1116    493\n",
       "   1144    490\n",
       "   37      479\n",
       "   234     490\n",
       "   1222    470\n",
       "   749     476\n",
       "   Name: feat_003, dtype: int64,\n",
       "   'y_train': 1156    467\n",
       "   1139    479\n",
       "   1945    498\n",
       "   1004    478\n",
       "   185     482\n",
       "   1145    490\n",
       "   1623    472\n",
       "   300     474\n",
       "   576     493\n",
       "   518     474\n",
       "   1479    486\n",
       "   1927    491\n",
       "   218     497\n",
       "   554     492\n",
       "   1017    502\n",
       "   1884    477\n",
       "   23      479\n",
       "   472     491\n",
       "   542     496\n",
       "   1852    495\n",
       "   950     477\n",
       "   44      487\n",
       "   1840    464\n",
       "   506     479\n",
       "   800     472\n",
       "   475     495\n",
       "   1574    478\n",
       "   139     483\n",
       "   198     486\n",
       "   132     487\n",
       "          ... \n",
       "   891     490\n",
       "   1656    476\n",
       "   600     487\n",
       "   1140    496\n",
       "   528     475\n",
       "   121     470\n",
       "   840     478\n",
       "   1023    482\n",
       "   1240    482\n",
       "   17      478\n",
       "   222     483\n",
       "   267     485\n",
       "   408     472\n",
       "   1977    480\n",
       "   1275    482\n",
       "   525     481\n",
       "   991     467\n",
       "   896     474\n",
       "   982     491\n",
       "   1092    475\n",
       "   535     486\n",
       "   1604    485\n",
       "   1201    497\n",
       "   1843    488\n",
       "   161     478\n",
       "   1526    483\n",
       "   603     493\n",
       "   1522    493\n",
       "   1068    481\n",
       "   1841    486\n",
       "   Name: feat_003, dtype: int64},\n",
       "  'test_score': 0.024,\n",
       "  'train_score': 0.24933333333333332,\n",
       "  'transformer': SelectKBest(k=5, score_func=<function f_classif at 0x114742a28>),\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'model': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "            metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "            weights='uniform'),\n",
       " 'test_score': 0.024,\n",
       " 'train_score': 0.24933333333333332,\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "general_model(KNeighborsClassifier(), KB_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNN_data_dict=general_model(KNeighborsClassifier(), KB_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/russellsasaki/anaconda/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "        [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "        [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "        ..., \n",
       "        [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "        [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "        [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       " 'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "        [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "        [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "        ..., \n",
       "        [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "        [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "        [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       " 'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "         [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "         [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "         ..., \n",
       "         [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "         [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "         [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "  'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "         [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "         [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "         ..., \n",
       "         [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "         [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "         [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "  'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "          [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "          [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "          ..., \n",
       "          [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "          [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "          [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "   'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "          [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "          [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "          ..., \n",
       "          [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "          [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "          [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "   'data_dict': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "           [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "           [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "           ..., \n",
       "           [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "           [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "           [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "    'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "           [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "           [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "           ..., \n",
       "           [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "           [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "           [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "    'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "             -0.23528695, -0.99733688],\n",
       "            [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "              0.53136977, -0.99733688],\n",
       "            [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "             -0.58028247,  1.00267023],\n",
       "            ..., \n",
       "            [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "             -1.15527501, -0.99733688],\n",
       "            [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "             -0.04362277, -0.99733688],\n",
       "            [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "              2.56301009, -0.99733688]]),\n",
       "     'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "             -0.61861531,  1.00267023],\n",
       "            [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "             -0.38861829,  1.00267023],\n",
       "            [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "              0.83803246,  1.00267023],\n",
       "            ..., \n",
       "            [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "              0.56970261, -0.99733688],\n",
       "            [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "              0.26303992,  1.00267023],\n",
       "            [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "             -0.65694814, -0.99733688]]),\n",
       "     'y_test': 1353    482\n",
       "     558     476\n",
       "     1566    486\n",
       "     76      488\n",
       "     980     477\n",
       "     1227    475\n",
       "     1466    470\n",
       "     606     492\n",
       "     682     472\n",
       "     460     492\n",
       "     1230    487\n",
       "     247     482\n",
       "     1322    483\n",
       "     1161    487\n",
       "     238     489\n",
       "     1719    478\n",
       "     417     462\n",
       "     186     482\n",
       "     557     475\n",
       "     1563    465\n",
       "     1391    475\n",
       "     796     488\n",
       "     294     485\n",
       "     1536    494\n",
       "     661     488\n",
       "     1547    476\n",
       "     203     486\n",
       "     1045    476\n",
       "     292     490\n",
       "     1272    476\n",
       "            ... \n",
       "     1687    474\n",
       "     1661    490\n",
       "     1690    480\n",
       "     394     488\n",
       "     1562    481\n",
       "     1457    482\n",
       "     1052    502\n",
       "     59      500\n",
       "     1725    486\n",
       "     793     475\n",
       "     1879    485\n",
       "     823     471\n",
       "     638     498\n",
       "     1450    492\n",
       "     1438    476\n",
       "     1791    492\n",
       "     1912    476\n",
       "     504     490\n",
       "     1948    494\n",
       "     1025    479\n",
       "     1298    482\n",
       "     60      490\n",
       "     268     479\n",
       "     471     488\n",
       "     1116    493\n",
       "     1144    490\n",
       "     37      479\n",
       "     234     490\n",
       "     1222    470\n",
       "     749     476\n",
       "     Name: feat_003, dtype: int64,\n",
       "     'y_train': 1156    467\n",
       "     1139    479\n",
       "     1945    498\n",
       "     1004    478\n",
       "     185     482\n",
       "     1145    490\n",
       "     1623    472\n",
       "     300     474\n",
       "     576     493\n",
       "     518     474\n",
       "     1479    486\n",
       "     1927    491\n",
       "     218     497\n",
       "     554     492\n",
       "     1017    502\n",
       "     1884    477\n",
       "     23      479\n",
       "     472     491\n",
       "     542     496\n",
       "     1852    495\n",
       "     950     477\n",
       "     44      487\n",
       "     1840    464\n",
       "     506     479\n",
       "     800     472\n",
       "     475     495\n",
       "     1574    478\n",
       "     139     483\n",
       "     198     486\n",
       "     132     487\n",
       "            ... \n",
       "     891     490\n",
       "     1656    476\n",
       "     600     487\n",
       "     1140    496\n",
       "     528     475\n",
       "     121     470\n",
       "     840     478\n",
       "     1023    482\n",
       "     1240    482\n",
       "     17      478\n",
       "     222     483\n",
       "     267     485\n",
       "     408     472\n",
       "     1977    480\n",
       "     1275    482\n",
       "     525     481\n",
       "     991     467\n",
       "     896     474\n",
       "     982     491\n",
       "     1092    475\n",
       "     535     486\n",
       "     1604    485\n",
       "     1201    497\n",
       "     1843    488\n",
       "     161     478\n",
       "     1526    483\n",
       "     603     493\n",
       "     1522    493\n",
       "     1068    481\n",
       "     1841    486\n",
       "     Name: feat_003, dtype: int64},\n",
       "    'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "    'y_test': 1353    482\n",
       "    558     476\n",
       "    1566    486\n",
       "    76      488\n",
       "    980     477\n",
       "    1227    475\n",
       "    1466    470\n",
       "    606     492\n",
       "    682     472\n",
       "    460     492\n",
       "    1230    487\n",
       "    247     482\n",
       "    1322    483\n",
       "    1161    487\n",
       "    238     489\n",
       "    1719    478\n",
       "    417     462\n",
       "    186     482\n",
       "    557     475\n",
       "    1563    465\n",
       "    1391    475\n",
       "    796     488\n",
       "    294     485\n",
       "    1536    494\n",
       "    661     488\n",
       "    1547    476\n",
       "    203     486\n",
       "    1045    476\n",
       "    292     490\n",
       "    1272    476\n",
       "           ... \n",
       "    1687    474\n",
       "    1661    490\n",
       "    1690    480\n",
       "    394     488\n",
       "    1562    481\n",
       "    1457    482\n",
       "    1052    502\n",
       "    59      500\n",
       "    1725    486\n",
       "    793     475\n",
       "    1879    485\n",
       "    823     471\n",
       "    638     498\n",
       "    1450    492\n",
       "    1438    476\n",
       "    1791    492\n",
       "    1912    476\n",
       "    504     490\n",
       "    1948    494\n",
       "    1025    479\n",
       "    1298    482\n",
       "    60      490\n",
       "    268     479\n",
       "    471     488\n",
       "    1116    493\n",
       "    1144    490\n",
       "    37      479\n",
       "    234     490\n",
       "    1222    470\n",
       "    749     476\n",
       "    Name: feat_003, dtype: int64,\n",
       "    'y_train': 1156    467\n",
       "    1139    479\n",
       "    1945    498\n",
       "    1004    478\n",
       "    185     482\n",
       "    1145    490\n",
       "    1623    472\n",
       "    300     474\n",
       "    576     493\n",
       "    518     474\n",
       "    1479    486\n",
       "    1927    491\n",
       "    218     497\n",
       "    554     492\n",
       "    1017    502\n",
       "    1884    477\n",
       "    23      479\n",
       "    472     491\n",
       "    542     496\n",
       "    1852    495\n",
       "    950     477\n",
       "    44      487\n",
       "    1840    464\n",
       "    506     479\n",
       "    800     472\n",
       "    475     495\n",
       "    1574    478\n",
       "    139     483\n",
       "    198     486\n",
       "    132     487\n",
       "           ... \n",
       "    891     490\n",
       "    1656    476\n",
       "    600     487\n",
       "    1140    496\n",
       "    528     475\n",
       "    121     470\n",
       "    840     478\n",
       "    1023    482\n",
       "    1240    482\n",
       "    17      478\n",
       "    222     483\n",
       "    267     485\n",
       "    408     472\n",
       "    1977    480\n",
       "    1275    482\n",
       "    525     481\n",
       "    991     467\n",
       "    896     474\n",
       "    982     491\n",
       "    1092    475\n",
       "    535     486\n",
       "    1604    485\n",
       "    1201    497\n",
       "    1843    488\n",
       "    161     478\n",
       "    1526    483\n",
       "    603     493\n",
       "    1522    493\n",
       "    1068    481\n",
       "    1841    486\n",
       "    Name: feat_003, dtype: int64},\n",
       "   'test_score': 0.024,\n",
       "   'train_score': 0.24933333333333332,\n",
       "   'transformer': SelectKBest(k=5, score_func=<function f_classif at 0x114742a28>),\n",
       "   'y_test': 1353    482\n",
       "   558     476\n",
       "   1566    486\n",
       "   76      488\n",
       "   980     477\n",
       "   1227    475\n",
       "   1466    470\n",
       "   606     492\n",
       "   682     472\n",
       "   460     492\n",
       "   1230    487\n",
       "   247     482\n",
       "   1322    483\n",
       "   1161    487\n",
       "   238     489\n",
       "   1719    478\n",
       "   417     462\n",
       "   186     482\n",
       "   557     475\n",
       "   1563    465\n",
       "   1391    475\n",
       "   796     488\n",
       "   294     485\n",
       "   1536    494\n",
       "   661     488\n",
       "   1547    476\n",
       "   203     486\n",
       "   1045    476\n",
       "   292     490\n",
       "   1272    476\n",
       "          ... \n",
       "   1687    474\n",
       "   1661    490\n",
       "   1690    480\n",
       "   394     488\n",
       "   1562    481\n",
       "   1457    482\n",
       "   1052    502\n",
       "   59      500\n",
       "   1725    486\n",
       "   793     475\n",
       "   1879    485\n",
       "   823     471\n",
       "   638     498\n",
       "   1450    492\n",
       "   1438    476\n",
       "   1791    492\n",
       "   1912    476\n",
       "   504     490\n",
       "   1948    494\n",
       "   1025    479\n",
       "   1298    482\n",
       "   60      490\n",
       "   268     479\n",
       "   471     488\n",
       "   1116    493\n",
       "   1144    490\n",
       "   37      479\n",
       "   234     490\n",
       "   1222    470\n",
       "   749     476\n",
       "   Name: feat_003, dtype: int64,\n",
       "   'y_train': 1156    467\n",
       "   1139    479\n",
       "   1945    498\n",
       "   1004    478\n",
       "   185     482\n",
       "   1145    490\n",
       "   1623    472\n",
       "   300     474\n",
       "   576     493\n",
       "   518     474\n",
       "   1479    486\n",
       "   1927    491\n",
       "   218     497\n",
       "   554     492\n",
       "   1017    502\n",
       "   1884    477\n",
       "   23      479\n",
       "   472     491\n",
       "   542     496\n",
       "   1852    495\n",
       "   950     477\n",
       "   44      487\n",
       "   1840    464\n",
       "   506     479\n",
       "   800     472\n",
       "   475     495\n",
       "   1574    478\n",
       "   139     483\n",
       "   198     486\n",
       "   132     487\n",
       "          ... \n",
       "   891     490\n",
       "   1656    476\n",
       "   600     487\n",
       "   1140    496\n",
       "   528     475\n",
       "   121     470\n",
       "   840     478\n",
       "   1023    482\n",
       "   1240    482\n",
       "   17      478\n",
       "   222     483\n",
       "   267     485\n",
       "   408     472\n",
       "   1977    480\n",
       "   1275    482\n",
       "   525     481\n",
       "   991     467\n",
       "   896     474\n",
       "   982     491\n",
       "   1092    475\n",
       "   535     486\n",
       "   1604    485\n",
       "   1201    497\n",
       "   1843    488\n",
       "   161     478\n",
       "   1526    483\n",
       "   603     493\n",
       "   1522    493\n",
       "   1068    481\n",
       "   1841    486\n",
       "   Name: feat_003, dtype: int64},\n",
       "  'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False),\n",
       "  'test_score': 0.042000000000000003,\n",
       "  'train_score': 0.048000000000000001,\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'model': GridSearchCV(cv=None, error_score='raise',\n",
       "        estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       "        fit_params={}, iid=True, n_jobs=1,\n",
       "        param_grid={'C': [0.01, 0.1, 1, 100.0]}, pre_dispatch='2*n_jobs',\n",
       "        refit=True, return_train_score=True, scoring=None, verbose=0),\n",
       " 'test_score': 0.042000000000000003,\n",
       " 'train_score': 0.048000000000000001,\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'C' : [1E-2, 1E-1, 1, 1E2]    \n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), param_grid=params)\n",
    "\n",
    "general_model(grid_search, LR_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.048000000000000001, 0.042000000000000003)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRGS=general_model(grid_search, LR_data_dict)\n",
    "LRGS['train_score'], LRGS['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "        [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "        [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "        ..., \n",
       "        [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "        [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "        [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       " 'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "        [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "        [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "        ..., \n",
       "        [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "        [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "        [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       " 'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "         [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "         [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "         ..., \n",
       "         [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "         [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "         [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "  'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "         [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "         [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "         ..., \n",
       "         [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "         [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "         [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "  'data_dictionary': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "          [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "          [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "          ..., \n",
       "          [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "          [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "          [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "   'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "          [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "          [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "          ..., \n",
       "          [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "          [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "          [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "   'data_dict': {'X_test': array([[ 0.15318359, -1.95859673,  0.42211221,  1.3302463 ,  1.45715892],\n",
       "           [-0.30563308, -0.30973479,  1.24074673,  0.71299446, -0.96243884],\n",
       "           [ 1.66727861, -0.9457244 ,  1.05882795,  1.91320637,  0.19476009],\n",
       "           ..., \n",
       "           [-0.81033142, -0.09773825,  0.60403099,  0.09574262, -0.22603952],\n",
       "           [-2.3703081 ,  0.60891686, -0.48748169, -0.17859153,  0.29995999],\n",
       "           [ 0.5661186 , -0.47462098, -2.03379134, -2.40755651, -1.27803855]]),\n",
       "    'X_train': array([[ 1.20846194,  0.70313755, -0.03268474,  0.85016154,  1.24675911],\n",
       "           [-1.17738475, -0.05062791, -0.57844109, -0.14429976, -2.22483767],\n",
       "           [ 0.24494693, -0.52173133,  0.42211221, -0.24717507,  1.14155921],\n",
       "           ..., \n",
       "           [-1.13150309, -1.06350025, -0.3965223 ,  0.30149323, -0.33123943],\n",
       "           [ 0.24494693,  1.33912715, -0.03268474,  0.54153561, -1.59363826],\n",
       "           [-0.12210641,  0.39692033,  0.42211221, -0.04142446,  1.56235882]]),\n",
       "    'data_dict': {'X_test': array([[ 0.19989201,  1.81985366, -1.45627385, ...,  0.40020192,\n",
       "             -0.23528695, -0.99733688],\n",
       "            [ 2.07387965,  0.24604925, -0.93824815, ..., -1.89551345,\n",
       "              0.53136977, -0.99733688],\n",
       "            [-1.9864269 ,  2.25516126, -0.29071602, ...,  0.82731175,\n",
       "             -0.58028247,  1.00267023],\n",
       "            ..., \n",
       "            [-0.11243926, -0.55759555, -0.42022245, ...,  1.4679765 ,\n",
       "             -1.15527501, -0.99733688],\n",
       "            [ 1.13688583, -0.55759555,  1.28926237, ...,  0.48028501,\n",
       "             -0.04362277, -0.99733688],\n",
       "            [-0.58093617, -1.36124036,  0.97844695, ...,  0.80061739,\n",
       "              2.56301009, -0.99733688]]),\n",
       "     'X_train': array([[-2.76725508,  1.31757566, -0.05760446, ...,  0.24003573,\n",
       "             -0.61861531,  1.00267023],\n",
       "            [ 0.35605765, -0.72502155, -0.91234687, ..., -0.10699101,\n",
       "             -0.38861829,  1.00267023],\n",
       "            [ 0.66838892, -1.89700356, -0.57563016, ...,  0.07986954,\n",
       "              0.83803246,  1.00267023],\n",
       "            ..., \n",
       "            [ 1.91771402,  0.17907885,  0.7194341 , ...,  0.24003573,\n",
       "              0.56970261, -0.99733688],\n",
       "            [-0.26860489,  0.41347525,  1.18565723, ..., -1.01459941,\n",
       "              0.26303992,  1.00267023],\n",
       "            [ 2.6985422 , -0.22274355,  1.15975594, ..., -0.40062903,\n",
       "             -0.65694814, -0.99733688]]),\n",
       "     'y_test': 1353    482\n",
       "     558     476\n",
       "     1566    486\n",
       "     76      488\n",
       "     980     477\n",
       "     1227    475\n",
       "     1466    470\n",
       "     606     492\n",
       "     682     472\n",
       "     460     492\n",
       "     1230    487\n",
       "     247     482\n",
       "     1322    483\n",
       "     1161    487\n",
       "     238     489\n",
       "     1719    478\n",
       "     417     462\n",
       "     186     482\n",
       "     557     475\n",
       "     1563    465\n",
       "     1391    475\n",
       "     796     488\n",
       "     294     485\n",
       "     1536    494\n",
       "     661     488\n",
       "     1547    476\n",
       "     203     486\n",
       "     1045    476\n",
       "     292     490\n",
       "     1272    476\n",
       "            ... \n",
       "     1687    474\n",
       "     1661    490\n",
       "     1690    480\n",
       "     394     488\n",
       "     1562    481\n",
       "     1457    482\n",
       "     1052    502\n",
       "     59      500\n",
       "     1725    486\n",
       "     793     475\n",
       "     1879    485\n",
       "     823     471\n",
       "     638     498\n",
       "     1450    492\n",
       "     1438    476\n",
       "     1791    492\n",
       "     1912    476\n",
       "     504     490\n",
       "     1948    494\n",
       "     1025    479\n",
       "     1298    482\n",
       "     60      490\n",
       "     268     479\n",
       "     471     488\n",
       "     1116    493\n",
       "     1144    490\n",
       "     37      479\n",
       "     234     490\n",
       "     1222    470\n",
       "     749     476\n",
       "     Name: feat_003, dtype: int64,\n",
       "     'y_train': 1156    467\n",
       "     1139    479\n",
       "     1945    498\n",
       "     1004    478\n",
       "     185     482\n",
       "     1145    490\n",
       "     1623    472\n",
       "     300     474\n",
       "     576     493\n",
       "     518     474\n",
       "     1479    486\n",
       "     1927    491\n",
       "     218     497\n",
       "     554     492\n",
       "     1017    502\n",
       "     1884    477\n",
       "     23      479\n",
       "     472     491\n",
       "     542     496\n",
       "     1852    495\n",
       "     950     477\n",
       "     44      487\n",
       "     1840    464\n",
       "     506     479\n",
       "     800     472\n",
       "     475     495\n",
       "     1574    478\n",
       "     139     483\n",
       "     198     486\n",
       "     132     487\n",
       "            ... \n",
       "     891     490\n",
       "     1656    476\n",
       "     600     487\n",
       "     1140    496\n",
       "     528     475\n",
       "     121     470\n",
       "     840     478\n",
       "     1023    482\n",
       "     1240    482\n",
       "     17      478\n",
       "     222     483\n",
       "     267     485\n",
       "     408     472\n",
       "     1977    480\n",
       "     1275    482\n",
       "     525     481\n",
       "     991     467\n",
       "     896     474\n",
       "     982     491\n",
       "     1092    475\n",
       "     535     486\n",
       "     1604    485\n",
       "     1201    497\n",
       "     1843    488\n",
       "     161     478\n",
       "     1526    483\n",
       "     603     493\n",
       "     1522    493\n",
       "     1068    481\n",
       "     1841    486\n",
       "     Name: feat_003, dtype: int64},\n",
       "    'transformer': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       "    'y_test': 1353    482\n",
       "    558     476\n",
       "    1566    486\n",
       "    76      488\n",
       "    980     477\n",
       "    1227    475\n",
       "    1466    470\n",
       "    606     492\n",
       "    682     472\n",
       "    460     492\n",
       "    1230    487\n",
       "    247     482\n",
       "    1322    483\n",
       "    1161    487\n",
       "    238     489\n",
       "    1719    478\n",
       "    417     462\n",
       "    186     482\n",
       "    557     475\n",
       "    1563    465\n",
       "    1391    475\n",
       "    796     488\n",
       "    294     485\n",
       "    1536    494\n",
       "    661     488\n",
       "    1547    476\n",
       "    203     486\n",
       "    1045    476\n",
       "    292     490\n",
       "    1272    476\n",
       "           ... \n",
       "    1687    474\n",
       "    1661    490\n",
       "    1690    480\n",
       "    394     488\n",
       "    1562    481\n",
       "    1457    482\n",
       "    1052    502\n",
       "    59      500\n",
       "    1725    486\n",
       "    793     475\n",
       "    1879    485\n",
       "    823     471\n",
       "    638     498\n",
       "    1450    492\n",
       "    1438    476\n",
       "    1791    492\n",
       "    1912    476\n",
       "    504     490\n",
       "    1948    494\n",
       "    1025    479\n",
       "    1298    482\n",
       "    60      490\n",
       "    268     479\n",
       "    471     488\n",
       "    1116    493\n",
       "    1144    490\n",
       "    37      479\n",
       "    234     490\n",
       "    1222    470\n",
       "    749     476\n",
       "    Name: feat_003, dtype: int64,\n",
       "    'y_train': 1156    467\n",
       "    1139    479\n",
       "    1945    498\n",
       "    1004    478\n",
       "    185     482\n",
       "    1145    490\n",
       "    1623    472\n",
       "    300     474\n",
       "    576     493\n",
       "    518     474\n",
       "    1479    486\n",
       "    1927    491\n",
       "    218     497\n",
       "    554     492\n",
       "    1017    502\n",
       "    1884    477\n",
       "    23      479\n",
       "    472     491\n",
       "    542     496\n",
       "    1852    495\n",
       "    950     477\n",
       "    44      487\n",
       "    1840    464\n",
       "    506     479\n",
       "    800     472\n",
       "    475     495\n",
       "    1574    478\n",
       "    139     483\n",
       "    198     486\n",
       "    132     487\n",
       "           ... \n",
       "    891     490\n",
       "    1656    476\n",
       "    600     487\n",
       "    1140    496\n",
       "    528     475\n",
       "    121     470\n",
       "    840     478\n",
       "    1023    482\n",
       "    1240    482\n",
       "    17      478\n",
       "    222     483\n",
       "    267     485\n",
       "    408     472\n",
       "    1977    480\n",
       "    1275    482\n",
       "    525     481\n",
       "    991     467\n",
       "    896     474\n",
       "    982     491\n",
       "    1092    475\n",
       "    535     486\n",
       "    1604    485\n",
       "    1201    497\n",
       "    1843    488\n",
       "    161     478\n",
       "    1526    483\n",
       "    603     493\n",
       "    1522    493\n",
       "    1068    481\n",
       "    1841    486\n",
       "    Name: feat_003, dtype: int64},\n",
       "   'test_score': 0.024,\n",
       "   'train_score': 0.24933333333333332,\n",
       "   'transformer': SelectKBest(k=5, score_func=<function f_classif at 0x114742a28>),\n",
       "   'y_test': 1353    482\n",
       "   558     476\n",
       "   1566    486\n",
       "   76      488\n",
       "   980     477\n",
       "   1227    475\n",
       "   1466    470\n",
       "   606     492\n",
       "   682     472\n",
       "   460     492\n",
       "   1230    487\n",
       "   247     482\n",
       "   1322    483\n",
       "   1161    487\n",
       "   238     489\n",
       "   1719    478\n",
       "   417     462\n",
       "   186     482\n",
       "   557     475\n",
       "   1563    465\n",
       "   1391    475\n",
       "   796     488\n",
       "   294     485\n",
       "   1536    494\n",
       "   661     488\n",
       "   1547    476\n",
       "   203     486\n",
       "   1045    476\n",
       "   292     490\n",
       "   1272    476\n",
       "          ... \n",
       "   1687    474\n",
       "   1661    490\n",
       "   1690    480\n",
       "   394     488\n",
       "   1562    481\n",
       "   1457    482\n",
       "   1052    502\n",
       "   59      500\n",
       "   1725    486\n",
       "   793     475\n",
       "   1879    485\n",
       "   823     471\n",
       "   638     498\n",
       "   1450    492\n",
       "   1438    476\n",
       "   1791    492\n",
       "   1912    476\n",
       "   504     490\n",
       "   1948    494\n",
       "   1025    479\n",
       "   1298    482\n",
       "   60      490\n",
       "   268     479\n",
       "   471     488\n",
       "   1116    493\n",
       "   1144    490\n",
       "   37      479\n",
       "   234     490\n",
       "   1222    470\n",
       "   749     476\n",
       "   Name: feat_003, dtype: int64,\n",
       "   'y_train': 1156    467\n",
       "   1139    479\n",
       "   1945    498\n",
       "   1004    478\n",
       "   185     482\n",
       "   1145    490\n",
       "   1623    472\n",
       "   300     474\n",
       "   576     493\n",
       "   518     474\n",
       "   1479    486\n",
       "   1927    491\n",
       "   218     497\n",
       "   554     492\n",
       "   1017    502\n",
       "   1884    477\n",
       "   23      479\n",
       "   472     491\n",
       "   542     496\n",
       "   1852    495\n",
       "   950     477\n",
       "   44      487\n",
       "   1840    464\n",
       "   506     479\n",
       "   800     472\n",
       "   475     495\n",
       "   1574    478\n",
       "   139     483\n",
       "   198     486\n",
       "   132     487\n",
       "          ... \n",
       "   891     490\n",
       "   1656    476\n",
       "   600     487\n",
       "   1140    496\n",
       "   528     475\n",
       "   121     470\n",
       "   840     478\n",
       "   1023    482\n",
       "   1240    482\n",
       "   17      478\n",
       "   222     483\n",
       "   267     485\n",
       "   408     472\n",
       "   1977    480\n",
       "   1275    482\n",
       "   525     481\n",
       "   991     467\n",
       "   896     474\n",
       "   982     491\n",
       "   1092    475\n",
       "   535     486\n",
       "   1604    485\n",
       "   1201    497\n",
       "   1843    488\n",
       "   161     478\n",
       "   1526    483\n",
       "   603     493\n",
       "   1522    493\n",
       "   1068    481\n",
       "   1841    486\n",
       "   Name: feat_003, dtype: int64},\n",
       "  'model': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform'),\n",
       "  'test_score': 0.042000000000000003,\n",
       "  'train_score': 0.048000000000000001,\n",
       "  'y_test': 1353    482\n",
       "  558     476\n",
       "  1566    486\n",
       "  76      488\n",
       "  980     477\n",
       "  1227    475\n",
       "  1466    470\n",
       "  606     492\n",
       "  682     472\n",
       "  460     492\n",
       "  1230    487\n",
       "  247     482\n",
       "  1322    483\n",
       "  1161    487\n",
       "  238     489\n",
       "  1719    478\n",
       "  417     462\n",
       "  186     482\n",
       "  557     475\n",
       "  1563    465\n",
       "  1391    475\n",
       "  796     488\n",
       "  294     485\n",
       "  1536    494\n",
       "  661     488\n",
       "  1547    476\n",
       "  203     486\n",
       "  1045    476\n",
       "  292     490\n",
       "  1272    476\n",
       "         ... \n",
       "  1687    474\n",
       "  1661    490\n",
       "  1690    480\n",
       "  394     488\n",
       "  1562    481\n",
       "  1457    482\n",
       "  1052    502\n",
       "  59      500\n",
       "  1725    486\n",
       "  793     475\n",
       "  1879    485\n",
       "  823     471\n",
       "  638     498\n",
       "  1450    492\n",
       "  1438    476\n",
       "  1791    492\n",
       "  1912    476\n",
       "  504     490\n",
       "  1948    494\n",
       "  1025    479\n",
       "  1298    482\n",
       "  60      490\n",
       "  268     479\n",
       "  471     488\n",
       "  1116    493\n",
       "  1144    490\n",
       "  37      479\n",
       "  234     490\n",
       "  1222    470\n",
       "  749     476\n",
       "  Name: feat_003, dtype: int64,\n",
       "  'y_train': 1156    467\n",
       "  1139    479\n",
       "  1945    498\n",
       "  1004    478\n",
       "  185     482\n",
       "  1145    490\n",
       "  1623    472\n",
       "  300     474\n",
       "  576     493\n",
       "  518     474\n",
       "  1479    486\n",
       "  1927    491\n",
       "  218     497\n",
       "  554     492\n",
       "  1017    502\n",
       "  1884    477\n",
       "  23      479\n",
       "  472     491\n",
       "  542     496\n",
       "  1852    495\n",
       "  950     477\n",
       "  44      487\n",
       "  1840    464\n",
       "  506     479\n",
       "  800     472\n",
       "  475     495\n",
       "  1574    478\n",
       "  139     483\n",
       "  198     486\n",
       "  132     487\n",
       "         ... \n",
       "  891     490\n",
       "  1656    476\n",
       "  600     487\n",
       "  1140    496\n",
       "  528     475\n",
       "  121     470\n",
       "  840     478\n",
       "  1023    482\n",
       "  1240    482\n",
       "  17      478\n",
       "  222     483\n",
       "  267     485\n",
       "  408     472\n",
       "  1977    480\n",
       "  1275    482\n",
       "  525     481\n",
       "  991     467\n",
       "  896     474\n",
       "  982     491\n",
       "  1092    475\n",
       "  535     486\n",
       "  1604    485\n",
       "  1201    497\n",
       "  1843    488\n",
       "  161     478\n",
       "  1526    483\n",
       "  603     493\n",
       "  1522    493\n",
       "  1068    481\n",
       "  1841    486\n",
       "  Name: feat_003, dtype: int64},\n",
       " 'model': GridSearchCV(cv=None, error_score='raise',\n",
       "        estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "   decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "   tol=0.001, verbose=False),\n",
       "        fit_params={}, iid=True, n_jobs=1,\n",
       "        param_grid={'C': [0.01, 0.1, 1, 100.0]}, pre_dispatch='2*n_jobs',\n",
       "        refit=True, return_train_score=True, scoring=None, verbose=0),\n",
       " 'test_score': 0.042000000000000003,\n",
       " 'train_score': 0.048000000000000001,\n",
       " 'y_test': 1353    482\n",
       " 558     476\n",
       " 1566    486\n",
       " 76      488\n",
       " 980     477\n",
       " 1227    475\n",
       " 1466    470\n",
       " 606     492\n",
       " 682     472\n",
       " 460     492\n",
       " 1230    487\n",
       " 247     482\n",
       " 1322    483\n",
       " 1161    487\n",
       " 238     489\n",
       " 1719    478\n",
       " 417     462\n",
       " 186     482\n",
       " 557     475\n",
       " 1563    465\n",
       " 1391    475\n",
       " 796     488\n",
       " 294     485\n",
       " 1536    494\n",
       " 661     488\n",
       " 1547    476\n",
       " 203     486\n",
       " 1045    476\n",
       " 292     490\n",
       " 1272    476\n",
       "        ... \n",
       " 1687    474\n",
       " 1661    490\n",
       " 1690    480\n",
       " 394     488\n",
       " 1562    481\n",
       " 1457    482\n",
       " 1052    502\n",
       " 59      500\n",
       " 1725    486\n",
       " 793     475\n",
       " 1879    485\n",
       " 823     471\n",
       " 638     498\n",
       " 1450    492\n",
       " 1438    476\n",
       " 1791    492\n",
       " 1912    476\n",
       " 504     490\n",
       " 1948    494\n",
       " 1025    479\n",
       " 1298    482\n",
       " 60      490\n",
       " 268     479\n",
       " 471     488\n",
       " 1116    493\n",
       " 1144    490\n",
       " 37      479\n",
       " 234     490\n",
       " 1222    470\n",
       " 749     476\n",
       " Name: feat_003, dtype: int64,\n",
       " 'y_train': 1156    467\n",
       " 1139    479\n",
       " 1945    498\n",
       " 1004    478\n",
       " 185     482\n",
       " 1145    490\n",
       " 1623    472\n",
       " 300     474\n",
       " 576     493\n",
       " 518     474\n",
       " 1479    486\n",
       " 1927    491\n",
       " 218     497\n",
       " 554     492\n",
       " 1017    502\n",
       " 1884    477\n",
       " 23      479\n",
       " 472     491\n",
       " 542     496\n",
       " 1852    495\n",
       " 950     477\n",
       " 44      487\n",
       " 1840    464\n",
       " 506     479\n",
       " 800     472\n",
       " 475     495\n",
       " 1574    478\n",
       " 139     483\n",
       " 198     486\n",
       " 132     487\n",
       "        ... \n",
       " 891     490\n",
       " 1656    476\n",
       " 600     487\n",
       " 1140    496\n",
       " 528     475\n",
       " 121     470\n",
       " 840     478\n",
       " 1023    482\n",
       " 1240    482\n",
       " 17      478\n",
       " 222     483\n",
       " 267     485\n",
       " 408     472\n",
       " 1977    480\n",
       " 1275    482\n",
       " 525     481\n",
       " 991     467\n",
       " 896     474\n",
       " 982     491\n",
       " 1092    475\n",
       " 535     486\n",
       " 1604    485\n",
       " 1201    497\n",
       " 1843    488\n",
       " 161     478\n",
       " 1526    483\n",
       " 603     493\n",
       " 1522    493\n",
       " 1068    481\n",
       " 1841    486\n",
       " Name: feat_003, dtype: int64}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_model(grid_search, KNN_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.048000000000000001, 0.042000000000000003)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNGS=general_model(grid_search, KNN_data_dict)\n",
    "KNNGS['train_score'], KNNGS['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
